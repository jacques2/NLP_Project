{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33ef3d5",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 1 ‚Äî Sexism Detection\n",
    "\n",
    "**Group members:** _[Jacopo Francesco Amoretti]_  \n",
    "\n",
    "---\n",
    "\n",
    "## Checklist di consegna\n",
    "- [ ] Task 1 ‚Äî Corpus (aggregazione majority vote, filtraggio EN, encoding label)\n",
    "- [ ] Task 2 ‚Äî Data Cleaning (emoji/hashtag/mention/url/symbols/quotes + lemmatizzazione)\n",
    "- [ ] Task 3 ‚Äî Text Encoding (GloVe + gestione OOV + embedding matrix)\n",
    "- [ ] Task 4 ‚Äî Modelli (BiLSTM baseline e stacked)\n",
    "- [ ] Task 5 ‚Äî Training & Evaluation (‚â• 3 seed, macro F1/Prec/Rec, avg ¬± std)\n",
    "- [ ] Task 6 ‚Äî Transformers (Twitter-roBERTa-base-hate + Trainer)\n",
    "- [ ] Task 7 ‚Äî Error Analysis (pattern errori, confusion/PR, esempi)\n",
    "- [ ] Task 8 ‚Äî Report (riassunto risultati, figure, tabella metriche)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d0cfe1",
   "metadata": {},
   "source": [
    "\n",
    "## Setup\n",
    "\n",
    "Eseguire questa sezione una volta all'inizio. Imposta seed, librerie e percorsi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56569fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup ok.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Import di base ===\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualizzazione/plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Metriche\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Opzionale: barra di progresso\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = lambda x: x\n",
    "\n",
    "# Seed e device\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Percorsi progetto (modificabili)\n",
    "DATA_DIR = Path('data')          # Deve contenere: train.json, val.json, test.json\n",
    "GLOVE_DIR = Path('glove')        # File come glove.6B.100d.txt (o altri)\n",
    "ARTIFACTS_DIR = Path('artifacts')# Dove salvare vocab, mapping, embedding matrix, ecc.\n",
    "MODELS_DIR = Path('models')\n",
    "RESULTS_DIR = Path('results')\n",
    "\n",
    "for d in [ARTIFACTS_DIR, MODELS_DIR, RESULTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Setup ok.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195575d",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1 ‚Äî Corpus\n",
    "\n",
    "**Obiettivo**  \n",
    "1. Caricare `train/val/test` JSON in DataFrame.  \n",
    "2. Majority voting su `labels_task2` ‚Üí nuova colonna `label` (rimuovere item senza maggioranza).  \n",
    "3. Filtrare `lang == 'en'`.  \n",
    "4. Tenere solo colonne: `id_EXIST`, `lang`, `tweet`, `label`.  \n",
    "5. Encoding `label` con mapping:\n",
    "```python\n",
    "label2id = {'-': 0, 'DIRECT': 1, 'JUDGEMENTAL': 2, 'REPORTED': 3}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe65cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Helper: majority vote su una lista di etichette ==\n",
    "from collections import Counter\n",
    "\n",
    "def majority_vote(labels):\n",
    "    # labels: lista di stringhe (e.g. ['DIRECT','DIRECT','REPORTED','-','JUDGEMENTAL','REPORTED'])\n",
    "    cnt = Counter(labels)\n",
    "    # Trova il valore con conteggio max\n",
    "    top = cnt.most_common()\n",
    "    if len(top) == 0:\n",
    "        return None, False\n",
    "    # Verifica unicit√† della maggioranza\n",
    "    if len(top) > 1 and top[0][1] == top[1][1]:\n",
    "        return None, False  # no clear majority\n",
    "    return top[0][0], True\n",
    "\n",
    "label2id = {'-': 0, 'DIRECT': 1, 'JUDGEMENTAL': 2, 'REPORTED': 3}\n",
    "id2label = {v:k for k,v in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564402da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Caricamento JSON in DataFrame ==\n",
    "# Atteso: tre file JSON con la struttura del repo del corso (train/val/test)\n",
    "# Sostituire i nomi file qui sotto con quelli effettivi se diversi.\n",
    "train_path = DATA_DIR / 'train.json'\n",
    "val_path   = DATA_DIR / 'val.json'\n",
    "test_path  = DATA_DIR / 'test.json'\n",
    "\n",
    "def load_exist_json(path: Path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # data √® un dict con chiavi id (string) -> record\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    return df\n",
    "\n",
    "df_train_raw = load_exist_json(train_path)\n",
    "df_val_raw   = load_exist_json(val_path)\n",
    "df_test_raw  = load_exist_json(test_path)\n",
    "\n",
    "print('Train raw:', df_train_raw.shape, '| Val raw:', df_val_raw.shape, '| Test raw:', df_test_raw.shape)\n",
    "df_train_raw.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba157af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Majority voting su labels_task2 e pulizia ==\n",
    "def apply_majority_and_filter(df):\n",
    "    mv_labels = []\n",
    "    keep_mask = []\n",
    "    for _, row in df.iterrows():\n",
    "        mv, ok = majority_vote(row['labels_task2'])\n",
    "        mv_labels.append(mv)\n",
    "        keep_mask.append(ok)\n",
    "    df = df.copy()\n",
    "    df['label'] = mv_labels\n",
    "    df = df[pd.Series(keep_mask).values]  # rimuove quelli senza maggioranza\n",
    "    return df\n",
    "\n",
    "df_train_mv = apply_majority_and_filter(df_train_raw)\n",
    "df_val_mv   = apply_majority_and_filter(df_val_raw)\n",
    "df_test_mv  = apply_majority_and_filter(df_test_raw)\n",
    "\n",
    "print('Dopo majority vote ->',\n",
    "      'Train:', df_train_mv.shape, 'Val:', df_val_mv.shape, 'Test:', df_test_mv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Filtro lingua EN e selezione colonne ==\n",
    "KEEP_COLS = ['id_EXIST', 'lang', 'tweet', 'label']\n",
    "\n",
    "def filter_and_select(df):\n",
    "    df = df[df['lang'] == 'en'].copy()\n",
    "    df = df[KEEP_COLS].copy()\n",
    "    return df\n",
    "\n",
    "df_train = filter_and_select(df_train_mv)\n",
    "df_val   = filter_and_select(df_val_mv)\n",
    "df_test  = filter_and_select(df_test_mv)\n",
    "\n",
    "print('Solo EN ->', 'Train:', df_train.shape, 'Val:', df_val.shape, 'Test:', df_test.shape)\n",
    "df_train.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb1423",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Encoding label ==\n",
    "def encode_labels(df):\n",
    "    df = df.copy()\n",
    "    df['label_id'] = df['label'].map(label2id)\n",
    "    return df\n",
    "\n",
    "df_train = encode_labels(df_train)\n",
    "df_val   = encode_labels(df_val)\n",
    "df_test  = encode_labels(df_test)\n",
    "\n",
    "print(df_train['label'].value_counts())\n",
    "df_train.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87690552",
   "metadata": {},
   "source": [
    "\n",
    "# Task 2 ‚Äî Data Cleaning\n",
    "\n",
    "**Requisiti**  \n",
    "- Rimuovere: emoji, hashtag (`#...`), mention (`@user`), URL, caratteri speciali/simboli, virgolette tipografiche.  \n",
    "- Lemmatizzazione (English).\n",
    "\n",
    "> **Nota:** potete usare `spaCy` con `en_core_web_sm` oppure `nltk`/`stanza`. Sotto trovate una pipeline base con spaCy; se il modello non √® installato, vedere i commenti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea201baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Pulizia testuale: regex base ==\n",
    "\n",
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "MENTION_RE = re.compile(r'@\\w+')\n",
    "HASHTAG_RE = re.compile(r'#\\w+')\n",
    "EMOJI_RE = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)  # rimuove emoji unicode\n",
    "SPECIAL_QUOTES_REPLACEMENTS = {\n",
    "    '‚Äú': '\"', '‚Äù': '\"', '‚Äò': \"'\", '‚Äô': \"'\",\n",
    "    '¬´': '\"', '¬ª': '\"', '‚Ä¶': '...'\n",
    "}\n",
    "\n",
    "def normalize_quotes(text: str):\n",
    "    for k, v in SPECIAL_QUOTES_REPLACEMENTS.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def basic_clean(text: str):\n",
    "    text = normalize_quotes(text)\n",
    "    text = URL_RE.sub(' ', text)\n",
    "    text = MENTION_RE.sub(' ', text)\n",
    "    text = HASHTAG_RE.sub(' ', text)\n",
    "    text = EMOJI_RE.sub(' ', text)\n",
    "    # rimuove caratteri non alfanumerici (teniamo punteggiatura minima .,!?,')\n",
    "    text = re.sub(r\"[^0-9A-Za-z'\\.,!\\?\\s]\", ' ', text)\n",
    "    # spazi multipli\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Test rapido\n",
    "print(basic_clean(\"Check this: https://ex.com @user #hashtag üòÖ ‚Äúquote‚Äù ‚Äî symbols!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a39320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Lemmatizzazione con spaCy (fallback graceful) ==\n",
    "USE_SPACY = True\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "    except Exception:\n",
    "        # Se non installato: !python -m spacy download en_core_web_sm\n",
    "        nlp = None\n",
    "        print(\"Attenzione: modello spaCy 'en_core_web_sm' non installato. Installare e rieseguire.\")\n",
    "except Exception:\n",
    "    USE_SPACY = False\n",
    "    nlp = None\n",
    "    print('spaCy non disponibile; saltando lemmatizzazione o usare altra libreria.')\n",
    "\n",
    "def lemmatize_en(texts):\n",
    "    if nlp is None:\n",
    "        return texts  # no-op\n",
    "    docs = nlp.pipe(texts, batch_size=512)\n",
    "    out = []\n",
    "    for doc in docs:\n",
    "        lemmas = [t.lemma_.lower() for t in doc if not t.is_space]\n",
    "        out.append(' '.join(lemmas))\n",
    "    return out\n",
    "\n",
    "def apply_clean_and_lemma(df, text_col='tweet'):\n",
    "    df = df.copy()\n",
    "    df['clean'] = df[text_col].astype(str).apply(basic_clean)\n",
    "    df['clean_lemma'] = lemmatize_en(df['clean'].tolist())\n",
    "    return df\n",
    "\n",
    "# Applicazione (potete ridurre a subset per debug rapido)\n",
    "df_train = apply_clean_and_lemma(df_train, 'tweet')\n",
    "df_val   = apply_clean_and_lemma(df_val, 'tweet')\n",
    "df_test  = apply_clean_and_lemma(df_test, 'tweet')\n",
    "\n",
    "df_train[['tweet','clean','clean_lemma']].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ca0af3",
   "metadata": {},
   "source": [
    "\n",
    "# Task 3 ‚Äî Text Encoding (GloVe + OOV)\n",
    "\n",
    "**Obiettivi**  \n",
    "- Tokenizzare `df_train['clean_lemma']` (o `clean` se non lemmatizzate).  \n",
    "- Costruire vocabolario: **tutti i token di train** (oppure unione `train ‚à™ GloVe`).  \n",
    "- Caricare GloVe e creare `embedding_matrix` con:\n",
    "  - token in GloVe ‚Üí vettore preaddestrato\n",
    "  - token OOV (in train ma non in GloVe) ‚Üí vettore custom (es. random)\n",
    "  - token ignoti in val/test ‚Üí `<UNK>` con embedding statico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7063fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Tokenizzazione molto semplice (whitespace + punteggiatura minima) ==\n",
    "TOKEN_RE = re.compile(r\"\\w+('[\\w]+)?\")\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return TOKEN_RE.findall(text.lower())\n",
    "\n",
    "def build_vocab_from_train(texts, min_freq=1):\n",
    "    from collections import Counter\n",
    "    c = Counter()\n",
    "    for t in texts:\n",
    "        for tok in simple_tokenize(t):\n",
    "            c[tok] += 1\n",
    "    vocab = {tok for tok, f in c.items() if f >= min_freq}\n",
    "    return vocab, c\n",
    "\n",
    "train_texts = df_train['clean_lemma'] if 'clean_lemma' in df_train.columns else df_train['clean']\n",
    "vocab_set, freq = build_vocab_from_train(train_texts.tolist(), min_freq=1)\n",
    "print('Vocab size (train):', len(vocab_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Caricamento GloVe ==\n",
    "# Esempio atteso: GLOVE_DIR / 'glove.6B.100d.txt'\n",
    "# Modificare EMB_DIM e filename secondo il file che avete.\n",
    "EMB_DIM = 100\n",
    "GLOVE_FILE = GLOVE_DIR / f'glove.6B.{EMB_DIM}d.txt'\n",
    "\n",
    "def load_glove(path):\n",
    "    emb = {}\n",
    "    if not path.exists():\n",
    "        print(f'ATTENZIONE: file GloVe non trovato: {path}. Caricheremo solo OOV random.')\n",
    "        return emb\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(' ')\n",
    "            w = parts[0]\n",
    "            vec = np.asarray(parts[1:], dtype=np.float32)\n",
    "            emb[w] = vec\n",
    "    print('GloVe vettori caricati:', len(emb))\n",
    "    return emb\n",
    "\n",
    "glove = load_glove(GLOVE_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b384080",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Costruzione vocabolario finale e embedding matrix ==\n",
    "# Strategie accettate: usare solo token di train oppure unione (train ‚à™ GloVe).\n",
    "USE_UNION_WITH_GLOVE = False\n",
    "\n",
    "SPECIAL_TOKENS = ['<PAD>', '<UNK>']\n",
    "token_list = sorted(vocab_set)\n",
    "if USE_UNION_WITH_GLOVE and len(glove) > 0:\n",
    "    token_list = sorted(set(token_list).union(set(glove.keys())))\n",
    "\n",
    "itos = SPECIAL_TOKENS + token_list\n",
    "stoi = {tok:i for i, tok in enumerate(itos)}\n",
    "\n",
    "def rand_vec(d):\n",
    "    # Inizializzazione per OOV\n",
    "    return np.random.normal(0, 0.1, size=(d,)).astype(np.float32)\n",
    "\n",
    "embedding_matrix = np.zeros((len(itos), EMB_DIM), dtype=np.float32)\n",
    "# <PAD> = zero vector per mascheramento\n",
    "embedding_matrix[stoi['<PAD>']] = np.zeros(EMB_DIM, dtype=np.float32)\n",
    "# <UNK> = random (statico)\n",
    "embedding_matrix[stoi['<UNK>']] = rand_vec(EMB_DIM)\n",
    "\n",
    "oov_count = 0\n",
    "for tok in token_list:\n",
    "    idx = stoi[tok]\n",
    "    if tok in glove:\n",
    "        embedding_matrix[idx] = glove[tok]\n",
    "    else:\n",
    "        embedding_matrix[idx] = rand_vec(EMB_DIM)\n",
    "        oov_count += 1\n",
    "\n",
    "print('Vocab totale:', len(itos), '| OOV (train vs GloVe):', oov_count)\n",
    "\n",
    "# Salvataggio mapping e embedding matrix per i task successivi\n",
    "np.save(ARTIFACTS_DIR / 'embedding_matrix.npy', embedding_matrix)\n",
    "pd.Series(itos).to_csv(ARTIFACTS_DIR / 'itos.csv', index=False)\n",
    "pd.Series(stoi).to_csv(ARTIFACTS_DIR / 'stoi.csv')\n",
    "print('Salvati embedding_matrix.npy, itos.csv, stoi.csv in', ARTIFACTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2deb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Conversione testo -> ID (per modelli con Embedding layer) ==\n",
    "MAX_LEN = 64  # modificabile\n",
    "PAD_ID = stoi['<PAD>']\n",
    "UNK_ID = stoi['<UNK>']\n",
    "\n",
    "def encode_text(text, max_len=MAX_LEN):\n",
    "    toks = simple_tokenize(text)\n",
    "    ids = [stoi.get(t, UNK_ID) for t in toks]\n",
    "    if len(ids) < max_len:\n",
    "        ids = ids + [PAD_ID] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "def encode_dataframe(df, text_col='clean_lemma'):\n",
    "    X = np.vstack([encode_text(t) for t in df[text_col].tolist()])\n",
    "    y = df['label_id'].values.astype(int)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = encode_dataframe(df_train)\n",
    "X_val,   y_val   = encode_dataframe(df_val)\n",
    "X_test,  y_test  = encode_dataframe(df_test)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c8bb6",
   "metadata": {},
   "source": [
    "\n",
    "# Task 4 ‚Äî Model Definition (BiLSTM)\n",
    "\n",
    "**Richiesto**  \n",
    "- **Baseline:** Bidirectional LSTM + Dense finale.  \n",
    "- **Stacked:** una seconda BiLSTM sopra la prima.  \n",
    "- Si pu√≤ usare Keras (consigliato) o PyTorch. Sotto trovate Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Keras BiLSTM models ==\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "NUM_CLASSES = 4  # '-', DIRECT, JUDGEMENTAL, REPORTED\n",
    "EMBED_TRAINABLE = False  # facoltativo\n",
    "\n",
    "def build_baseline_bilstm(vocab_size, emb_dim, embedding_matrix, max_len=64):\n",
    "    inp = layers.Input(shape=(max_len,), name='input_ids')\n",
    "    emb = layers.Embedding(input_dim=vocab_size,\n",
    "                           output_dim=emb_dim,\n",
    "                           weights=[embedding_matrix],\n",
    "                           trainable=EMBED_TRAINABLE,\n",
    "                           mask_zero=True,\n",
    "                           name='encoder_embedding')(inp)\n",
    "    x = layers.Bidirectional(layers.LSTM(128))(emb)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    model = models.Model(inp, out, name='bilstm_baseline')\n",
    "    return model\n",
    "\n",
    "def build_stacked_bilstm(vocab_size, emb_dim, embedding_matrix, max_len=64):\n",
    "    inp = layers.Input(shape=(max_len,), name='input_ids')\n",
    "    emb = layers.Embedding(input_dim=vocab_size,\n",
    "                           output_dim=emb_dim,\n",
    "                           weights=[embedding_matrix],\n",
    "                           trainable=EMBED_TRAINABLE,\n",
    "                           mask_zero=True,\n",
    "                           name='encoder_embedding')(inp)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(emb)\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    model = models.Model(inp, out, name='bilstm_stacked')\n",
    "    return model\n",
    "\n",
    "VOCAB_SIZE = embedding_matrix.shape[0]\n",
    "EMB_DIM = embedding_matrix.shape[1]\n",
    "\n",
    "baseline = build_baseline_bilstm(VOCAB_SIZE, EMB_DIM, embedding_matrix, MAX_LEN)\n",
    "stacked  = build_stacked_bilstm(VOCAB_SIZE, EMB_DIM, embedding_matrix, MAX_LEN)\n",
    "\n",
    "baseline.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6c1d6",
   "metadata": {},
   "source": [
    "\n",
    "# Task 5 ‚Äî Training & Evaluation\n",
    "\n",
    "**Requisiti**  \n",
    "- Allenare **‚â• 3 seed** per ciascun modello.  \n",
    "- Valutare su **validation** con macro F1/Precision/Recall.  \n",
    "- Riportare **media ¬± std**.  \n",
    "- Scegliere il modello migliore con **macro F1** su validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e531c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Loop di training multi-seed (scheletro) ==\n",
    "def train_and_eval(model_fn, X_tr, y_tr, X_va, y_va, seeds=[1337, 2025, 42], epochs=5, batch_size=64):\n",
    "    histories = []\n",
    "    scores = []\n",
    "    for s in seeds:\n",
    "        tf.keras.utils.set_random_seed(s)\n",
    "        model = model_fn(VOCAB_SIZE, EMB_DIM, embedding_matrix, MAX_LEN)\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        h = model.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n",
    "                      epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "        histories.append(h.history)\n",
    "        # Valutazione dettagliata\n",
    "        y_pred = np.argmax(model.predict(X_va), axis=1)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_va, y_pred, average='macro', zero_division=0)\n",
    "        scores.append({'seed': s, 'precision': prec, 'recall': rec, 'f1': f1})\n",
    "    return histories, pd.DataFrame(scores)\n",
    "\n",
    "# Esempio d'uso (commentato per evitare esecuzione involontaria in ambienti lenti):\n",
    "# hist_base, df_scores_base = train_and_eval(build_baseline_bilstm, X_train, y_train, X_val, y_val)\n",
    "# df_scores_base, df_scores_base.mean(), df_scores_base.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e0fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Funzioni utili per report metriche e confusion matrix ==\n",
    "def evaluate_predictions(y_true, y_pred, labels_map=id2label):\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    df_rep = pd.DataFrame(report).T\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=sorted(labels_map.keys()))\n",
    "    return df_rep, cm\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest')\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703619a",
   "metadata": {},
   "source": [
    "\n",
    "# Task 6 ‚Äî Transformers (Twitter-roBERTa-base-hate)\n",
    "\n",
    "Modello: **cardiffnlp/twitter-roberta-base-hate**  \n",
    "Passi:\n",
    "1. Tokenizzazione con tokenizer HF.\n",
    "2. Preparazione `Dataset` (train/val/test).\n",
    "3. `Trainer` con metrica macro F1.\n",
    "4. Valutazione su **test** con le **stesse metriche** dei modelli LSTM.\n",
    "\n",
    "> **Nota:** richiede internet per scaricare il modello; se non disponibile, preparare in locale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Scheletro per HuggingFace Trainer ==\n",
    "# Commentato per evitare errori in ambienti offline.\n",
    "\n",
    "# from datasets import Dataset\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# MODEL_NAME = \"cardiffnlp/twitter-roberta-base-hate\"\n",
    "\n",
    "# # preparazione dataset HF\n",
    "# def to_hf_dataset(df, text_col='clean_lemma'):\n",
    "#    return Dataset.from_pandas(df[[text_col, 'label_id']].rename(columns={text_col:'text','label_id':'label'}))\n",
    "\n",
    "# ds_train = to_hf_dataset(df_train)\n",
    "# ds_val   = to_hf_dataset(df_val)\n",
    "# ds_test  = to_hf_dataset(df_test)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# def tokenize_fn(ex):\n",
    "#     return tokenizer(ex['text'], truncation=True, padding='max_length', max_length=64)\n",
    "# ds_train = ds_train.map(tokenize_fn, batched=True)\n",
    "# ds_val   = ds_val.map(tokenize_fn, batched=True)\n",
    "# ds_test  = ds_test.map(tokenize_fn, batched=True)\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4)\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     preds = logits.argmax(axis=-1)\n",
    "#     prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "#     return {'macro_f1': f1, 'macro_precision': prec, 'macro_recall': rec}\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir='hf_outputs',\n",
    "#     evaluation_strategy='epoch',\n",
    "#     save_strategy='epoch',\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model='macro_f1',\n",
    "#     logging_steps=50,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=ds_train,\n",
    "#     eval_dataset=ds_val,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# # trainer.train()\n",
    "# # eval_results = trainer.evaluate(ds_test)\n",
    "# # eval_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e061d0",
   "metadata": {},
   "source": [
    "\n",
    "# Task 7 ‚Äî Error Analysis\n",
    "\n",
    "**Suggerimenti**  \n",
    "- Confusion matrix per modello migliore.  \n",
    "- Precision/Recall per classe (tabella).  \n",
    "- Esempi di misclassificazioni tipiche (shortlist).  \n",
    "- Nota su OOV, imbalance, differenze LSTM vs Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# == Esempio: raccolta errori tipici (da completare dopo il training) ==\n",
    "# y_true = y_val\n",
    "# y_pred = y_pred_val  # ottenuto dal modello migliore\n",
    "# err_idx = np.where(y_true != y_pred)[0][:20]\n",
    "# df_errors = df_val.iloc[err_idx][['tweet','clean_lemma','label','label_id']].copy()\n",
    "# df_errors['pred_label'] = [id2label[i] for i in y_pred[err_idx]]\n",
    "# df_errors.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846a202",
   "metadata": {},
   "source": [
    "\n",
    "# Task 8 ‚Äî Report (Template)\n",
    "\n",
    "**Da fare nel PDF (max 2 pagine):**\n",
    "- Breve descrizione del dataset e preprocessing (Task 1‚Äì2).\n",
    "- Descrizione encoding e vocabolario/GloVe (Task 3).\n",
    "- Modelli LSTM (architettura/hyperparam) e Transformer (Task 4‚Äì6).\n",
    "- Tabella con metriche macro (avg ¬± std) su validation; metriche finali su test.\n",
    "- Figure: learning curves (acc/loss) e confusion matrix.\n",
    "- Error analysis: pattern ricorrenti, proposte di miglioramento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3051607",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Note operative\n",
    "- **Eseguite** le celle in ordine dall'alto verso il basso.\n",
    "- **Aggiornate** i percorsi a `data/` e `glove/` se necessario.\n",
    "- **Documentate** scelte e iperparametri nelle celle Markdown.\n",
    "- **Pulite** output superflui prima della consegna.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
