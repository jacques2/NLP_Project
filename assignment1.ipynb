{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "706bd4b0",
      "metadata": {
        "id": "706bd4b0"
      },
      "source": [
        "\n",
        "# Assignment 1 — Sexism Detection (EXIST 2023 Task 2)\n",
        "\n",
        "**Group members:** Jacopo Francesco Amoretti, Roberto Frabetti, Ivo Rambaldi\n",
        "\n",
        "---\n",
        "\n",
        "## Delivery checklist\n",
        "- [ ] Task 1 — Corpus (majority vote aggregation, EN filter, label encoding)\n",
        "- [ ] Task 2 — Data Cleaning (emoji/hashtag/mention/url/symbols/quotes + lemmatization)\n",
        "- [ ] Task 3 — Text Encoding (GloVe + OOV handling + embedding matrix)\n",
        "- [ ] Task 4 — Models (BiLSTM baseline and stacked)\n",
        "- [ ] Task 5 — Training & Evaluation (≥ 3 seeds, macro F1/Prec/Rec, avg ± std)\n",
        "- [ ] Task 6 — Transformers (Twitter-roBERTa-base-hate + Trainer)\n",
        "- [ ] Task 7 — Error Analysis (error patterns, confusion/PR, examples)\n",
        "- [ ] Task 8 — Report (summary of results, figures, metrics table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20e68b9",
      "metadata": {
        "id": "b20e68b9"
      },
      "source": [
        "\n",
        "## Setup\n",
        "\n",
        "Run this once at the beginning. It sets seeds, libraries, and project paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ee4bfcef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee4bfcef",
        "outputId": "52343010-181c-4bdd-a6bf-c789b0de3189"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jacopoamoretti/miniforge3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === Basic imports ===\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Visualization/plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "# Optional: progress bar\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except Exception:\n",
        "    tqdm = lambda x: x\n",
        "\n",
        "# Seed and device\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Project paths (adjust as needed)\n",
        "DATA_DIR = Path('data')          # Should contain: train.json, val.json, test.json\n",
        "GLOVE_DIR = Path('glove')        # Files like glove.6B.100d.txt \n",
        "ARTIFACTS_DIR = Path('artifacts') # Save vocab, mappings, embedding matrix, etc.\n",
        "MODELS_DIR = Path('models')\n",
        "RESULTS_DIR = Path('results')\n",
        "\n",
        "for d in [ARTIFACTS_DIR, MODELS_DIR, RESULTS_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Setup complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a70aa95",
      "metadata": {
        "id": "9a70aa95"
      },
      "source": [
        "\n",
        "# Task 1 — Corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7685b788",
      "metadata": {
        "id": "7685b788"
      },
      "outputs": [],
      "source": [
        "\n",
        "# == Majority vote on a list of labels ==\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def majority_vote(labels):\n",
        "    cnt = Counter(labels)\n",
        "    top = cnt.most_common() # Most frequent labels in descending order\n",
        "    if len(top) == 0:\n",
        "        return None, False\n",
        "    if len(top) > 1 and top[0][1] == top[1][1]:\n",
        "        return None, False  # If no clear majority\n",
        "    return top[0][0], True # Return majority label\n",
        "\n",
        "# Mapping between textual and numerical label representations\n",
        "label2id = {'-': 0, 'DIRECT': 1, 'JUDGEMENTAL': 2, 'REPORTED': 3}\n",
        "id2label = {v:k for k,v in label2id.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f6a7ac6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "f6a7ac6f",
        "outputId": "754e1fb7-62d2-41a9-fa0f-acbe65951b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train raw: (6920, 11) | Val raw: (726, 11) | Test raw: (312, 11)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_EXIST</th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>number_annotators</th>\n",
              "      <th>annotators</th>\n",
              "      <th>gender_annotators</th>\n",
              "      <th>age_annotators</th>\n",
              "      <th>labels_task1</th>\n",
              "      <th>labels_task2</th>\n",
              "      <th>labels_task3</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100001</th>\n",
              "      <td>100001</td>\n",
              "      <td>es</td>\n",
              "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
              "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
              "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100002</th>\n",
              "      <td>100002</td>\n",
              "      <td>es</td>\n",
              "      <td>@ultimonomada_ Si comicsgate se parece en algo...</td>\n",
              "      <td>6</td>\n",
              "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
              "      <td>[F, F, F, M, M, M]</td>\n",
              "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
              "      <td>[NO, NO, NO, NO, YES, NO]</td>\n",
              "      <td>[-, -, -, -, DIRECT, -]</td>\n",
              "      <td>[[-], [-], [-], [-], [OBJECTIFICATION], [-]]</td>\n",
              "      <td>TRAIN_ES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id_EXIST lang                                              tweet  \\\n",
              "100001   100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
              "100002   100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
              "\n",
              "        number_annotators                                         annotators  \\\n",
              "100001                  6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
              "100002                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
              "\n",
              "         gender_annotators                          age_annotators  \\\n",
              "100001  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "100002  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
              "\n",
              "                         labels_task1  \\\n",
              "100001  [YES, YES, NO, YES, YES, YES]   \n",
              "100002      [NO, NO, NO, NO, YES, NO]   \n",
              "\n",
              "                                             labels_task2  \\\n",
              "100001  [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
              "100002                            [-, -, -, -, DIRECT, -]   \n",
              "\n",
              "                                             labels_task3     split  \n",
              "100001  [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES  \n",
              "100002       [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  TRAIN_ES  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# == Load JSON into a DataFrame ==\n",
        "\n",
        "train_path = Path('data') / 'training.json'\n",
        "val_path   = Path('data') / 'validation.json'\n",
        "test_path  = Path('data') / 'test.json'\n",
        "\n",
        "def loadJson(path: Path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    df = pd.DataFrame.from_dict(data, orient='index') # Each key is an index\n",
        "    return df\n",
        "\n",
        "# Load raw splits into DataFrames\n",
        "df_train_raw = loadJson(train_path)\n",
        "df_val_raw   = loadJson(val_path)\n",
        "df_test_raw  = loadJson(test_path)\n",
        "\n",
        "print('Train raw:', df_train_raw.shape, '| Val raw:', df_val_raw.shape, '| Test raw:', df_test_raw.shape)\n",
        "df_train_raw.head(2) # Display first 2 rows of training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "85f7cb5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85f7cb5a",
        "outputId": "3365ff8a-9a9c-47f1-a840-22e584006577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After adding label column and drop ambiguous labels: Train: (6065, 12) Val: (630, 12) Test: (280, 12)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# == Majority voting on labels_task2 and drop ambiguous labels ==\n",
        "\n",
        "def apply_majority_and_drop(df):\n",
        "    mv_labels = []\n",
        "    keep_mask = []\n",
        "    for _, row in df.iterrows():\n",
        "        mv, evaluation = majority_vote(row['labels_task2'])  # Apply majority vote to each row\n",
        "        mv_labels.append(mv)\n",
        "        keep_mask.append(evaluation)\n",
        "    df = df.copy()\n",
        "    df['label'] = mv_labels  # Add final label column\n",
        "    df = df[pd.Series(keep_mask).values] # Drop rows without clear majority\n",
        "    return df\n",
        "\n",
        "# Apply to all dataset splits\n",
        "df_train_mv = apply_majority_and_drop(df_train_raw)\n",
        "df_val_mv   = apply_majority_and_drop(df_val_raw)\n",
        "df_test_mv  = apply_majority_and_drop(df_test_raw)\n",
        "\n",
        "print('After adding label column and drop ambiguous labels:',\n",
        "      'Train:', df_train_mv.shape, 'Val:', df_val_mv.shape, 'Test:', df_test_mv.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4c94608d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "4c94608d",
        "outputId": "e1201442-616d-40c6-ae61-c2de6ae1cc49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EN only: Train: (2873, 4) Val: (150, 4) Test: (280, 4)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_EXIST</th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200001</th>\n",
              "      <td>200001</td>\n",
              "      <td>en</td>\n",
              "      <td>FFS! How about laying the blame on the bastard...</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200002</th>\n",
              "      <td>200002</td>\n",
              "      <td>en</td>\n",
              "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
              "      <td>REPORTED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200003</th>\n",
              "      <td>200003</td>\n",
              "      <td>en</td>\n",
              "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
              "      <td>REPORTED</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id_EXIST lang                                              tweet  \\\n",
              "200001   200001   en  FFS! How about laying the blame on the bastard...   \n",
              "200002   200002   en  Writing a uni essay in my local pub with a cof...   \n",
              "200003   200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
              "\n",
              "           label  \n",
              "200001         -  \n",
              "200002  REPORTED  \n",
              "200003  REPORTED  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# == EN filter and column selection ==\n",
        "\n",
        "keep_columns = ['id_EXIST', 'lang', 'tweet', 'label']\n",
        "\n",
        "def filter_and_select(df):\n",
        "    df = df[df['lang'] == 'en'].copy()  # Keep only English tweets\n",
        "    df = df[keep_columns].copy()        # Select relevant columns\n",
        "    return df\n",
        "\n",
        "# Apply filtering to all dataset splits\n",
        "df_train = filter_and_select(df_train_mv)\n",
        "df_val   = filter_and_select(df_val_mv)\n",
        "df_test  = filter_and_select(df_test_mv)\n",
        "\n",
        "print('EN only:', 'Train:', df_train.shape, 'Val:', df_val.shape, 'Test:', df_test.shape)\n",
        "df_train.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "25271b27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "25271b27",
        "outputId": "0a9305ea-b0b0-454b-b30a-7218a3e37e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "-              2014\n",
            "DIRECT          537\n",
            "REPORTED        184\n",
            "JUDGEMENTAL     138\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_EXIST</th>\n",
              "      <th>lang</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>label_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200001</th>\n",
              "      <td>200001</td>\n",
              "      <td>en</td>\n",
              "      <td>FFS! How about laying the blame on the bastard...</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200002</th>\n",
              "      <td>200002</td>\n",
              "      <td>en</td>\n",
              "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
              "      <td>REPORTED</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200003</th>\n",
              "      <td>200003</td>\n",
              "      <td>en</td>\n",
              "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
              "      <td>REPORTED</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id_EXIST lang                                              tweet  \\\n",
              "200001   200001   en  FFS! How about laying the blame on the bastard...   \n",
              "200002   200002   en  Writing a uni essay in my local pub with a cof...   \n",
              "200003   200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
              "\n",
              "           label  label_id  \n",
              "200001         -         0  \n",
              "200002  REPORTED         3  \n",
              "200003  REPORTED         3  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# == Label encoding ==\n",
        "\n",
        "def encode_labels(df):\n",
        "    df = df.copy()\n",
        "    df['label_id'] = df['label'].map(label2id)  # Convert text labels to numeric IDs\n",
        "    return df\n",
        "\n",
        "# Apply label encoding to all dataset splits\n",
        "df_train = encode_labels(df_train)\n",
        "df_val   = encode_labels(df_val)\n",
        "df_test  = encode_labels(df_test)\n",
        "\n",
        "print(df_train['label'].value_counts())  # Check label distribution\n",
        "df_train.head(3)     # Inspect encoded DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "402502c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "402502c3",
        "outputId": "b815d80b-d67c-4ef7-8990-f9fd30a38044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved post-Task1 datasets to 'results/' directory.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# == Save post-Task1 datasets for quick reuse ==\n",
        "\n",
        "df_train[['id_EXIST','lang','tweet','label','label_id']].to_csv('results/train_task1.csv', index=False)\n",
        "df_val[['id_EXIST','lang','tweet','label','label_id']].to_csv('results/val_task1.csv', index=False)\n",
        "df_test[['id_EXIST','lang','tweet','label','label_id']].to_csv('results/test_task1.csv', index=False)\n",
        "print(\"Saved post-Task1 datasets to 'results/' directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a2719fb",
      "metadata": {
        "id": "4a2719fb"
      },
      "source": [
        "\n",
        "# Task 2 — Data Cleaning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "eb741167",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb741167",
        "outputId": "8c9ff72c-60bb-48e8-985d-2145a3d37c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Check this https try.com quote symbols\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# == Text cleaning: basic regex ==\n",
        "\n",
        "import re\n",
        "\n",
        "URL = re.compile(r'https?://\\S+|www\\.\\S+') # Match URLs\n",
        "MENTION = re.compile(r'@\\w+') # Match @ symbol\n",
        "HASHTAG = re.compile(r'#\\w+') # Match hashtags\n",
        "EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE) # Match emojis\n",
        "SPECIAL_QUOTES_REPLACEMENTS = { # Special quotes to normal quotes\n",
        "    '“': '\"', '”': '\"', '‘': \"'\", '’': \"'\",\n",
        "    '«': '\"', '»': '\"', '…': '...'\n",
        "}\n",
        "\n",
        "def normalize_quotes(text: str):\n",
        "    for k, v in SPECIAL_QUOTES_REPLACEMENTS.items():\n",
        "        text = text.replace(k, v)\n",
        "    return text\n",
        "\n",
        "def basic_clean(text: str):\n",
        "    text = normalize_quotes(text)                           # Standardize quotes\n",
        "    text = URL.sub(' ', text)                               # Remove URLs\n",
        "    text = MENTION.sub(' ', text)                           # Remove mentions\n",
        "    text = HASHTAG.sub(' ', text)                           # Remove hashtags\n",
        "    text = EMOJI.sub(' ', text)                             # Remove emojis\n",
        "    text = re.sub(r\"[^0-9A-Za-z'\\.,!\\?\\s]\", ' ', text)      # Remove unwanted symbols\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()                # Normalize spaces\n",
        "    return text\n",
        "\n",
        "print(basic_clean(\"Check this: https:/try.com @user #hashtag 👍🏻 “quote” — symbols\")) # Check the function correctness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e30c0a07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "e30c0a07",
        "outputId": "00385f73-4951-496c-d943-7cd8b393a8b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>clean</th>\n",
              "      <th>clean_lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200001</th>\n",
              "      <td>FFS! How about laying the blame on the bastard...</td>\n",
              "      <td>FFS! How about laying the blame on the bastard...</td>\n",
              "      <td>ffs ! how about lay the blame on the bastard w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200002</th>\n",
              "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
              "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
              "      <td>write a uni essay in my local pub with a coffe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200003</th>\n",
              "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
              "      <td>it is 2021 not 1921. I dont appreciate that on...</td>\n",
              "      <td>it be 2021 not 1921 . i do not appreciate that...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    tweet  \\\n",
              "200001  FFS! How about laying the blame on the bastard...   \n",
              "200002  Writing a uni essay in my local pub with a cof...   \n",
              "200003  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
              "\n",
              "                                                    clean  \\\n",
              "200001  FFS! How about laying the blame on the bastard...   \n",
              "200002  Writing a uni essay in my local pub with a cof...   \n",
              "200003  it is 2021 not 1921. I dont appreciate that on...   \n",
              "\n",
              "                                              clean_lemma  \n",
              "200001  ffs ! how about lay the blame on the bastard w...  \n",
              "200002  write a uni essay in my local pub with a coffe...  \n",
              "200003  it be 2021 not 1921 . i do not appreciate that...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# == Lemmatization with spaCy ==\n",
        "\n",
        "USE_SPACY = True\n",
        "\n",
        "# Load spaCy model and verify if it's available\n",
        "try:\n",
        "    import spacy\n",
        "    try:\n",
        "        nlp = spacy.load('en_core_web_sm', disable=['ner'])  # Load lightweight English model\n",
        "    except Exception:\n",
        "        nlp = None\n",
        "        print(\"Warning: spaCy model 'en_core_web_sm' not installed. Install it and re-run.\")\n",
        "except Exception:\n",
        "    USE_SPACY = False\n",
        "    nlp = None\n",
        "    print('spaCy not available; skipping lemmatization or use another library.')\n",
        "\n",
        "def lemmatize_en(texts):\n",
        "    if nlp is None:\n",
        "        return texts    # Skip if model not available\n",
        "    docs = nlp.pipe(texts, batch_size=512) # Process texts in batches for efficiency\n",
        "    out = []\n",
        "    for doc in docs:\n",
        "        lemmas = [t.lemma_.lower() for t in doc if not t.is_space] # Creating lemmas list\n",
        "        out.append(' '.join(lemmas))\n",
        "    return out\n",
        "\n",
        "def apply_clean_and_lemma(df, text_col='tweet'):\n",
        "    df = df.copy()\n",
        "    df['clean'] = df[text_col].astype(str).apply(basic_clean)    # Apply regex-based cleaning\n",
        "    df['clean_lemma'] = lemmatize_en(df['clean'].tolist())  # Lemmatize cleaned text\n",
        "    return df\n",
        "\n",
        "# Apply cleaning and lemmatization to all splits\n",
        "df_train = apply_clean_and_lemma(df_train, 'tweet')\n",
        "df_val   = apply_clean_and_lemma(df_val, 'tweet')\n",
        "df_test  = apply_clean_and_lemma(df_test, 'tweet')\n",
        "\n",
        "df_train[['tweet','clean','clean_lemma']].head(3)   # Inspect transformation results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23c84a95",
      "metadata": {
        "id": "23c84a95"
      },
      "source": [
        "\n",
        "# Task 3 — Text Encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9170a2c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9170a2c1",
        "outputId": "66be3392-c4c3-4fb2-b900-2ea4595cad77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size (train): 9074\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('be', 3793),\n",
              " ('the', 2648),\n",
              " ('to', 1998),\n",
              " ('i', 1992),\n",
              " ('a', 1915),\n",
              " ('and', 1657),\n",
              " ('of', 1393),\n",
              " ('not', 1313),\n",
              " ('you', 1115),\n",
              " ('it', 984),\n",
              " ('that', 917),\n",
              " ('do', 906),\n",
              " ('in', 894),\n",
              " ('have', 780),\n",
              " ('woman', 755),\n",
              " ('for', 728),\n",
              " ('they', 558),\n",
              " ('this', 546),\n",
              " ('on', 535),\n",
              " ('like', 515)]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TOKEN_RE = re.compile(r\"\\w+(?:'\\w+)?\") # Basic regex tokenizer (keeps apostrophes within words)\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    return TOKEN_RE.findall(str(text).lower())  # Tokenize and lowercase text\n",
        "\n",
        "def build_vocab_from_train(texts, min_freq=1):\n",
        "    from collections import Counter\n",
        "    c = Counter()\n",
        "    for t in texts:\n",
        "        for tok in simple_tokenize(t):\n",
        "            c[tok] += 1 # Count token frequency\n",
        "    vocab = {tok for tok, f in c.items() if f >= min_freq}  # Keep tokens above frequency threshold\n",
        "    return vocab, c\n",
        "\n",
        "# Build vocabulary from training texts\n",
        "train_texts = df_train['clean_lemma'] if 'clean_lemma' in df_train.columns else df_train['clean']\n",
        "vocab_set, freq = build_vocab_from_train(train_texts.tolist(), min_freq=1)\n",
        "\n",
        "print('Vocab size (train):', len(vocab_set))\n",
        "\n",
        "# Display top 20 most frequent tokens\n",
        "from collections import Counter\n",
        "tokens = [tok for t in train_texts for tok in simple_tokenize(t)]\n",
        "Counter(tokens).most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0a6b4597",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a6b4597",
        "outputId": "12634619-2546-448f-9558-5ef3203ff34d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GloVe ready at: glove/glove.twitter.27B.100d.txt\n",
            "Loaded GloVe vectors: 1193515\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# == Download and load GloVe (Twitter 27B) ==\n",
        "\n",
        "import os, pathlib, zipfile, urllib.request\n",
        "\n",
        "EMB_DIM = 100\n",
        "GLOVE_DIR = pathlib.Path(\"glove\")\n",
        "GLOVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "GLOVE_FILE = GLOVE_DIR / f\"glove.twitter.27B.{EMB_DIM}d.txt\"\n",
        "\n",
        "# Download and extract GloVe embeddings if not already present\n",
        "if not GLOVE_FILE.exists():\n",
        "    url = \"https://nlp.stanford.edu/data/glove.twitter.27B.zip\"\n",
        "    zip_path = GLOVE_DIR / \"glove.twitter.27B.zip\"\n",
        "    print(\"Downloading:\", url)\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "    with zipfile.ZipFile(zip_path) as zf:\n",
        "        zf.extract(GLOVE_FILE.name, GLOVE_DIR)\n",
        "    os.remove(zip_path)\n",
        "print(\"GloVe ready at:\", GLOVE_FILE)\n",
        "\n",
        "\n",
        "GLOVE_FILE = Path('glove') / f'glove.twitter.27B.{EMB_DIM}d.txt'\n",
        "\n",
        "def load_glove(path):\n",
        "    emb = {}\n",
        "    if not path.exists():\n",
        "        print(f'WARNING: GloVe file not found: {path}. Will initialize OOV randomly.')\n",
        "        return emb\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split(' ')\n",
        "            w = parts[0]\n",
        "            vec = np.asarray(parts[1:], dtype=np.float32)   # Convert embedding values to float array\n",
        "            emb[w] = vec\n",
        "    print('Loaded GloVe vectors:', len(emb))\n",
        "    return emb\n",
        "\n",
        "\n",
        "# Load pre-trained embeddings into dictionary\n",
        "glove = load_glove(GLOVE_FILE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7c3341c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c3341c9",
        "outputId": "1a2fb00a-966e-435d-b957-81018c6ff153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total vocab: 9076 | OOV (train vs GloVe): 1067\n",
            "Saved embedding_matrix.npy, itos.csv, stoi.csv to artifacts/\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# == Build embedding matrix and save artifacts ==\n",
        "\n",
        "SPECIAL_TOKENS = ['<PAD>', '<UNK>']\n",
        "token_list = sorted(vocab_set)\n",
        "\n",
        "itos = SPECIAL_TOKENS + token_list  # Index → token mapping\n",
        "stoi = {tok:i for i, tok in enumerate(itos)}    # Token → index mapping\n",
        "\n",
        "def rand_vec(d):\n",
        "    return np.random.normal(0, 0.1, size=(d,)).astype(np.float32)   # Random vector for OOV tokens\n",
        "\n",
        "# Initialize embedding matrix\n",
        "embedding_matrix = np.zeros((len(itos), EMB_DIM), dtype=np.float32)\n",
        "embedding_matrix[stoi['<PAD>']] = np.zeros(EMB_DIM, dtype=np.float32)\n",
        "embedding_matrix[stoi['<UNK>']] = rand_vec(EMB_DIM)\n",
        "\n",
        "\n",
        "# Fill embedding matrix using GloVe, random for OOV tokens\n",
        "oov_count = 0\n",
        "for tok in token_list:\n",
        "    idx = stoi[tok]\n",
        "    if tok in glove:\n",
        "        embedding_matrix[idx] = glove[tok]\n",
        "    else:\n",
        "        embedding_matrix[idx] = rand_vec(EMB_DIM)\n",
        "        oov_count += 1\n",
        "\n",
        "print('Total vocab:', len(itos), '| OOV (train vs GloVe):', oov_count)\n",
        "\n",
        "# Save embeddings and vocabulary files for reuse\n",
        "np.save(Path('artifacts') / 'embedding_matrix.npy', embedding_matrix)\n",
        "import pandas as pd\n",
        "pd.Series(itos).to_csv(Path('artifacts') / 'itos.csv', index=False)\n",
        "pd.Series(stoi).to_csv(Path('artifacts') / 'stoi.csv')\n",
        "print('Saved embedding_matrix.npy, itos.csv, stoi.csv to artifacts/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7e03e8de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e03e8de",
        "outputId": "aebb6f52-04b0-48c8-8d42-209c0d29b3ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((2873, 64), (150, 64), (280, 64))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# == Encode text into token IDs ==\n",
        "\n",
        "MAX_LEN = 64\n",
        "PAD_ID = stoi['<PAD>']\n",
        "UNK_ID = stoi['<UNK>']\n",
        "\n",
        "def encode_text(text, max_len=MAX_LEN):\n",
        "    toks = simple_tokenize(text)\n",
        "    ids = [stoi.get(t, UNK_ID) for t in toks]    # Convert tokens to IDs, use UNK_ID for unseen tokens\n",
        "    if len(ids) < max_len:\n",
        "        ids = ids + [PAD_ID] * (max_len - len(ids)) # Pad shorter sequences\n",
        "    else:\n",
        "        ids = ids[:max_len] # Truncate longer sequences\n",
        "    return ids\n",
        "\n",
        "def encode_dataframe(df, text_col='clean_lemma'):\n",
        "    X = np.vstack([encode_text(t) for t in df[text_col].tolist()])  # Encode all texts\n",
        "    y = df['label_id'].values.astype(int)\n",
        "    return X, y\n",
        "\n",
        "# Encode datasets into numeric form\n",
        "X_train, y_train = encode_dataframe(df_train)\n",
        "X_val,   y_val   = encode_dataframe(df_val)\n",
        "X_test,  y_test  = encode_dataframe(df_test)\n",
        "\n",
        "X_train.shape, X_val.shape, X_test.shape  # Check encoded matrix dimensions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf66d6c3",
      "metadata": {
        "id": "bf66d6c3"
      },
      "source": [
        "\n",
        "# Task 4 — Model Definition (BiLSTM)\n",
        "\n",
        "**Required**  \n",
        "- **Baseline:** Bidirectional LSTM + final Dense.  \n",
        "- **Stacked:** add a second BiLSTM on top.  \n",
        "- Keras example below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "10e6cac1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "10e6cac1",
        "outputId": "cc87af31-7a6e-4841-c1eb-aaa395dc7d5b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "NUM_CLASSES = 4\n",
        "EMBED_TRAINABLE = False\n",
        "\n",
        "def build_baseline_bilstm(vocab_size, emb_dim, embedding_matrix, max_len=64):\n",
        "    inp = layers.Input(shape=(max_len,), name='input_ids')\n",
        "    emb = layers.Embedding(input_dim=vocab_size,\n",
        "                           output_dim=emb_dim,\n",
        "                           weights=[embedding_matrix],\n",
        "                           trainable=EMBED_TRAINABLE,\n",
        "                           mask_zero=True,\n",
        "                           name='encoder_embedding')(inp)\n",
        "    x = layers.Bidirectional(layers.LSTM(128))(emb)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    out = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "    model = models.Model(inp, out, name='bilstm_baseline')\n",
        "    return model\n",
        "\n",
        "def build_stacked_bilstm(vocab_size, emb_dim, embedding_matrix, max_len=64):\n",
        "    inp = layers.Input(shape=(max_len,), name='input_ids')\n",
        "    emb = layers.Embedding(input_dim=vocab_size,\n",
        "                           output_dim=emb_dim,\n",
        "                           weights=[embedding_matrix],\n",
        "                           trainable=EMBED_TRAINABLE,\n",
        "                           mask_zero=True,\n",
        "                           name='encoder_embedding')(inp)\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(emb)\n",
        "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "    model = models.Model(inp, out, name='bilstm_stacked')\n",
        "    return model\n",
        "\n",
        "VOCAB_SIZE = embedding_matrix.shape[0]\n",
        "EMB_DIM = embedding_matrix.shape[1]\n",
        "\n",
        "baseline = build_baseline_bilstm(VOCAB_SIZE, EMB_DIM, embedding_matrix, MAX_LEN)\n",
        "stacked  = build_stacked_bilstm(VOCAB_SIZE, EMB_DIM, embedding_matrix, MAX_LEN)\n",
        "\n",
        "baseline.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "648f5c05",
      "metadata": {
        "id": "648f5c05"
      },
      "source": [
        "\n",
        "# Task 5 — Training & Evaluation\n",
        "\n",
        "Train with ≥ 3 seeds, evaluate on validation (macro F1/Precision/Recall), and report mean ± std.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "934f4465",
      "metadata": {
        "id": "934f4465"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "def train_and_eval(model_fn, X_tr, y_tr, X_va, y_va, seeds=[1337, 2025, 42], epochs=5, batch_size=64):\n",
        "    histories = []\n",
        "    scores = []\n",
        "    for s in seeds:\n",
        "        tf.keras.utils.set_random_seed(s)\n",
        "        model = model_fn(VOCAB_SIZE, EMB_DIM, embedding_matrix, MAX_LEN)\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        h = model.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n",
        "                      epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "        histories.append(h.history)\n",
        "        y_pred = np.argmax(model.predict(X_va), axis=1)\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(y_va, y_pred, average='macro', zero_division=0)\n",
        "        scores.append({'seed': s, 'precision': prec, 'recall': rec, 'f1': f1})\n",
        "    return histories, pd.DataFrame(scores)\n",
        "\n",
        "# Example (commented):\n",
        "# hist_base, df_scores_base = train_and_eval(build_baseline_bilstm, X_train, y_train, X_val, y_val)\n",
        "# df_scores_base, df_scores_base.mean(), df_scores_base.std()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde4c7fd",
      "metadata": {
        "id": "dde4c7fd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_predictions(y_true, y_pred, labels_map=id2label):\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    df_rep = pd.DataFrame(report).T\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=sorted(labels_map.keys()))\n",
        "    return df_rep, cm\n",
        "\n",
        "def plot_confusion_matrix(cm, labels):\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, ax = plt.subplots(figsize=(5,5))\n",
        "    im = ax.imshow(cm, interpolation='nearest')\n",
        "    ax.set_xticks(range(len(labels)))\n",
        "    ax.set_yticks(range(len(labels)))\n",
        "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "    ax.set_yticklabels(labels)\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('True')\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, cm[i, j], ha='center', va='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97566a65",
      "metadata": {
        "id": "97566a65"
      },
      "source": [
        "\n",
        "# Task 6 — Transformers (Twitter-roBERTa-base-hate)\n",
        "\n",
        "Model: **cardiffnlp/twitter-roberta-base-hate**  \n",
        "- Tokenize with HF tokenizer, prepare `Dataset`, use `Trainer` with macro F1, evaluate on test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1dddf66",
      "metadata": {
        "id": "a1dddf66"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Skeleton for HF Trainer (commented for offline environments)\n",
        "# from datasets import Dataset\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "# from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# MODEL_NAME = \"cardiffnlp/twitter-roberta-base-hate\"\n",
        "\n",
        "# def to_hf_dataset(df, text_col='clean_lemma'):\n",
        "#     return Dataset.from_pandas(df[[text_col, 'label_id']].rename(columns={text_col:'text','label_id':'label'}))\n",
        "\n",
        "# ds_train = to_hf_dataset(df_train)\n",
        "# ds_val   = to_hf_dataset(df_val)\n",
        "# ds_test  = to_hf_dataset(df_test)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "# def tokenize_fn(ex):\n",
        "#     return tokenizer(ex['text'], truncation=True, padding='max_length', max_length=64)\n",
        "# ds_train = ds_train.map(tokenize_fn, batched=True)\n",
        "# ds_val   = ds_val.map(tokenize_fn, batched=True)\n",
        "# ds_test  = ds_test.map(tokenize_fn, batched=True)\n",
        "\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4)\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     logits, labels = eval_pred\n",
        "#     preds = logits.argmax(axis=-1)\n",
        "#     prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
        "#     return {'macro_f1': f1, 'macro_precision': prec, 'macro_recall': rec}\n",
        "\n",
        "# args = TrainingArguments(\n",
        "#     output_dir='hf_outputs',\n",
        "#     evaluation_strategy='epoch',\n",
        "#     save_strategy='epoch',\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=32,\n",
        "#     per_device_eval_batch_size=64,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "#     load_best_model_at_end=True,\n",
        "#     metric_for_best_model='macro_f1',\n",
        "#     logging_steps=50,\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=args,\n",
        "#     train_dataset=ds_train,\n",
        "#     eval_dataset=ds_val,\n",
        "#     compute_metrics=compute_metrics,\n",
        "# )\n",
        "\n",
        "# # trainer.train()\n",
        "# # eval_results = trainer.evaluate(ds_test)\n",
        "# # eval_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e199bd1f",
      "metadata": {
        "id": "e199bd1f"
      },
      "source": [
        "\n",
        "# Task 7 — Error Analysis\n",
        "\n",
        "Suggestions: confusion matrix for the best model, per-class Precision/Recall table, typical misclassified examples, comments on OOV and imbalance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e10e081c",
      "metadata": {
        "id": "e10e081c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example (fill after training):\n",
        "# y_true = y_val\n",
        "# y_pred = y_pred_val\n",
        "# err_idx = np.where(y_true != y_pred)[0][:20]\n",
        "# df_errors = df_val.iloc[err_idx][['tweet','clean_lemma','label','label_id']].copy()\n",
        "# df_errors['pred_label'] = [id2label[i] for i in y_pred[err_idx]]\n",
        "# df_errors.head(10)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11 (nlp)",
      "language": "python",
      "name": "nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
